{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0aStgWSO0E0E"
   },
   "source": [
    "# **SIGMOID Modeling and Evaluation Notebook**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "- Answer Business Requirement 2: Develop a Machine Learning model to classify cherry leaves as Healthy or Infected, enabling the prediction of powdery mildew presence.\n",
    "\n",
    "## Inputs\n",
    "\n",
    "Dataset\n",
    "- inputs/mildew_dataset/cherry-leaves/train\n",
    "- inputs/mildew_dataset/cherry-leaves/validation\n",
    "- inputs/mildew_dataset/cherry-leaves/test\n",
    "\n",
    "Precomputed Features (from Data Visualization Notebook)\n",
    "- Image Shape Standardization → 128x128x3 for consistency across models.\n",
    "- Class Distribution Analysis → Ensures balanced dataset splits.\n",
    "- Pixel Intensity Distribution → Confirms brightness variations relevant for classification.\n",
    "\n",
    "## Outputs\n",
    "\n",
    "### Data Processing & Visualization\n",
    "- **Dataset Distribution Plot** → Confirms balanced data split across training, validation, and test sets.  \n",
    "- **Data Augmentation Visualization** → Showcases applied transformations (rotation, flipping, zooming).  \n",
    "\n",
    "### Model Training & Evaluation\n",
    "- **Baseline CNN Model** → Initial implementation for benchmarking.  \n",
    "- **Hyperparameter-Tuned CNN** → Optimized model through manual adjustments.  \n",
    "- **Best Model Selection** → Chooses the final model based on test accuracy and generalization ability.  \n",
    "- **Saved Trained Models** → Final model stored for deployment.  \n",
    "\n",
    "### Model Performance & Explainability\n",
    "- **Learning Curves** → Visualizes loss and accuracy trends over epochs.  \n",
    "- **Histograms** → Displays predicted probability distributions.  \n",
    "- **Overfitting & Generalization Check** → Assesses potential overfitting using accuracy and loss gaps.  \n",
    "- **Confusion Matrices** → Shows classification performance for train, validation, and test sets.  \n",
    "- **Classification Reports** → Provides precision, recall, and F1-score analysis.  \n",
    "- **ROC Curves** → Evaluates model performance using Receiver Operating Characteristic analysis.  \n",
    "- **Business Goal Validation** → Confirms if the model meets the required accuracy threshold.  \n",
    "\n",
    "## Additional Comments\n",
    "\n",
    "- **Business Impact:** Enables early detection of powdery mildew, reducing manual inspection and improving monitoring.  \n",
    "- **Data-Driven Improvements:** Model refinements were based on data insights, ensuring balanced class distribution.  \n",
    "- **Deployment:** The optimized model is ready for Streamlit integration for real-world use.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9uWZXH9LwoQg"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cqP-UeN-z3i2"
   },
   "source": [
    "# Set Data Directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Necessary Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wZfF_j-Bz3i4",
    "outputId": "66943449-1436-4c3d-85c7-b85f9f78349b"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.image import imread"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Working Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd= os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TwHsQRWjz3i9",
    "outputId": "86849db3-cd2f-4cc5-ebb8-2d0caafa1a2c"
   },
   "outputs": [],
   "source": [
    "os.chdir('/workspaces/mildew-detector')\n",
    "print(\"You set a new current directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M_xPk_Ijz3i-"
   },
   "source": [
    "#### Confirm the new current directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vz3S-_kjz3jA",
    "outputId": "00b79ae4-75d0-4a96-d193-ac9ef9847ea2"
   },
   "outputs": [],
   "source": [
    "work_dir = os.getcwd()\n",
    "work_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-mavJ8DibrcQ"
   },
   "source": [
    "## Set Input Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set input directories\n",
    "my_data_dir = 'inputs/mildew_dataset/cherry-leaves'\n",
    "train_path = os.path.join(my_data_dir, 'train')\n",
    "val_path = os.path.join(my_data_dir, 'validation')\n",
    "test_path = os.path.join(my_data_dir, 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Output Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = 'draft_sigmoid'\n",
    "file_path = f'outputs/{version}'\n",
    "\n",
    "if 'outputs' in os.listdir(work_dir) and version in os.listdir(work_dir + '/outputs'):\n",
    "    print('Old version is already available create a new version.')\n",
    "    pass\n",
    "else:\n",
    "    os.makedirs(name=file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Label Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the labels for the images\n",
    "labels = os.listdir(train_path)\n",
    "print('Label for the images are', labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Image Shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import saved image shape embedding\n",
    "import joblib\n",
    "version = 'v1'\n",
    "image_shape = joblib.load(filename=f\"outputs/{version}/image_shape.pkl\")\n",
    "image_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of Images in Train, Test and Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create an empty dictionary\n",
    "data = {\n",
    "    'Set': [],\n",
    "    'Label': [],\n",
    "    'Frequency': []\n",
    "}\n",
    "\n",
    "# Define dataset folders\n",
    "folders = ['train', 'validation', 'test']\n",
    "\n",
    "# Loop through each dataset split and count images\n",
    "for folder in folders:\n",
    "    for label in labels:\n",
    "        path = os.path.join(my_data_dir, folder, label)\n",
    "        num_images = len(os.listdir(path)) if os.path.exists(path) else 0  \n",
    "        data['Set'].append(folder)\n",
    "        data['Label'].append(label)\n",
    "        data['Frequency'].append(num_images)\n",
    "        print(f\" {folder}/{label}: {num_images} images\")\n",
    "\n",
    "# Convert dictionary to DataFrame\n",
    "df_freq = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bar Chart - Image Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"whitegrid\")\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.barplot(data=df_freq, x='Set', y='Frequency', hue='Label')\n",
    "plt.title(\"Image Distribution in Dataset\")\n",
    "plt.xlabel(\"Dataset Split\")\n",
    "plt.ylabel(\"Number of Images\")\n",
    "plt.savefig(f'{file_path}/labels_distribution.png', bbox_inches='tight', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import TensorFlow/Keras ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augment Training, Validation, and Test Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Initialize ImageDataGenerator for Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Augmentation for Training Set\n",
    "augmented_image_data = ImageDataGenerator(rotation_range=20,\n",
    "                                          width_shift_range=0.10,\n",
    "                                          height_shift_range=0.10,\n",
    "                                          shear_range=0.1,\n",
    "                                          zoom_range=0.1,\n",
    "                                          horizontal_flip=True,\n",
    "                                          vertical_flip=True,\n",
    "                                          fill_mode='nearest',\n",
    "                                          rescale=1./255\n",
    "                                          )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Augment Training Image Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32  # Set batch size\n",
    "train_set = augmented_image_data.flow_from_directory(train_path,\n",
    "                                                     target_size=image_shape[:2],\n",
    "                                                     color_mode='rgb',\n",
    "                                                     batch_size=batch_size,\n",
    "                                                     class_mode='binary',\n",
    "                                                     shuffle=True\n",
    "                                                     )\n",
    "\n",
    "train_set.class_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Augment Validation Image Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_set = ImageDataGenerator(rescale=1./255).flow_from_directory(val_path,\n",
    "                                                                        target_size=image_shape[:2],\n",
    "                                                                        color_mode='rgb',\n",
    "                                                                        batch_size=batch_size,\n",
    "                                                                        class_mode='binary',\n",
    "                                                                        shuffle=False\n",
    "                                                                        )\n",
    "\n",
    "validation_set.class_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Augment Test Image Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = ImageDataGenerator(rescale=1./255).flow_from_directory(test_path,\n",
    "                                                                  target_size=image_shape[:2],\n",
    "                                                                  color_mode='rgb',\n",
    "                                                                  batch_size=batch_size,\n",
    "                                                                  class_mode='binary',\n",
    "                                                                  shuffle=False\n",
    "                                                                  )\n",
    "\n",
    "test_set.class_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZY3l0-AxO93d"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization of Augmented Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Augmented Training Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(3):\n",
    "    img, label = next(train_set)\n",
    "    print(img.shape)  # (1,256,256,3)\n",
    "    plt.imshow(img[0])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Augmented Validation and Test Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(3):\n",
    "    img, label = next(validation_set)\n",
    "    print(img.shape)  # (1,256,256,3)\n",
    "    plt.imshow(img[0])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(3):\n",
    "    img, label = next(test_set)\n",
    "    print(img.shape)  # (1,256,256,3)\n",
    "    plt.imshow(img[0])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Class Indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(value=train_set.class_indices,\n",
    "            filename=f\"{file_path}/class_indices.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Multiple Augmented Images in a Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_augmented_images_grid(data_generator, num_images=10):\n",
    "    \"\"\"Displays a grid of augmented images to visualize transformation effects.\"\"\"\n",
    "    img_batch, label_batch = next(data_generator)\n",
    "\n",
    "    fig, axes = plt.subplots(2, num_images // 2, figsize=(15, 6))\n",
    "    \n",
    "    for i in range(num_images):\n",
    "        ax = axes[i // (num_images // 2), i % (num_images // 2)]\n",
    "        ax.imshow(img_batch[i])\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "    plt.suptitle(\"Augmented Image Variations (Training Set)\")\n",
    "    plt.show()\n",
    "\n",
    "# Display the augmented image grid\n",
    "plot_augmented_images_grid(train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import (\n",
    "    Conv2D,\n",
    "    MaxPooling2D,\n",
    "    Flatten,\n",
    "    Dense,\n",
    "    Dropout,\n",
    "    BatchNormalization,\n",
    "    Input,\n",
    ")\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adagrad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network with Sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define L2 regularization strength\n",
    "l2_lambda = 0.005  \n",
    "\n",
    "# Create Sigmoid CNN Model with Fine-Tuned Regularization\n",
    "model_sigmoid = Sequential(\n",
    "    [\n",
    "        Input(shape=(128, 128, 3)),\n",
    "        Conv2D(16, (3, 3), activation=\"relu\", kernel_regularizer=l2(l2_lambda)),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Conv2D(32, (3, 3), activation=\"relu\", kernel_regularizer=l2(l2_lambda)),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Conv2D(64, (3, 3), activation=\"relu\", kernel_regularizer=l2(l2_lambda)),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Flatten(),\n",
    "        Dense(128, activation=\"relu\", kernel_regularizer=l2(l2_lambda)),\n",
    "        Dropout(0.2),  \n",
    "        Dense(1, activation=\"sigmoid\"),  # Sigmoid for binary classification\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Compile Model\n",
    "model_sigmoid.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),  \n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Summary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print model summary\n",
    "model_sigmoid.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model summary to a text file\n",
    "with open(\"outputs/draft_sigmoid/model_summary.txt\", \"w\") as f:\n",
    "    model.summary(print_fn=lambda x: f.write(x + \"\\n\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Early Stopping Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required callbacks\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Set EarlyStopping callback\n",
    "early_stop = EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit CNN Model for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the base CNN model\n",
    "history_sigmoid = model_sigmoid.fit(\n",
    "    train_set,\n",
    "    epochs=20,\n",
    "    steps_per_epoch=len(train_set.classes) // batch_size,\n",
    "    validation_data=validation_set,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained base CNN model\n",
    "model_sigmoid.save(\"outputs/draft_sigmoid/mildew_detector_sigmoid.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Performance & Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    f1_score,\n",
    "    accuracy_score,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Saved Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "model = load_model(\"outputs/draft_sigmoid/mildew_detector_sigmoid.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model Evaluation\n",
    "evaluation = model.evaluate(test_set, batch_size=batch_size)\n",
    "print(\"Model accuracy: {:.2f}%\".format(evaluation[1] * 100))\n",
    "print(\"Model Loss: \", evaluation[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Accuracy Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correctly obtain true labels\n",
    "y_true = test_set.labels\n",
    "\n",
    "# Obtain model predictions\n",
    "preds = model.predict(test_set)\n",
    "y_pred = np.argmax(preds, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_history_sigmoid = pd.DataFrame(history_sigmoid.history)\n",
    "df_history_sigmoid.to_csv(\"outputs/draft_sigmoid/history_sigmoid.csv\", index=False)\n",
    "print(\"Sigmoid CNN training history saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "output_dir = \"outputs/draft_sigmoid\"\n",
    "\n",
    "# Set Seaborn style\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Loss Curve\n",
    "df_history_sigmoid[[\"loss\", \"val_loss\"]].plot(style=\".-\")\n",
    "plt.title(\"Loss - Sigmoid CNN\")\n",
    "plt.legend([\"Training Loss\", \"Validation Loss\"])\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\n",
    "    \"outputs/draft_sigmoid/model_training_losses.png\", bbox_inches=\"tight\", dpi=150\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "# Accuracy Curve\n",
    "df_history_sigmoid[[\"accuracy\", \"val_accuracy\"]].plot(style=\".-\")\n",
    "plt.title(\"Accuracy - Sigmoid CNN\")\n",
    "plt.legend([\"Training Accuracy\", \"Validation Accuracy\"])\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\n",
    "    \"outputs/draft_sigmoid/model_training_acc.png\", bbox_inches=\"tight\", dpi=150\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "output_dir = \"outputs/draft_sigmoid\"\n",
    "\n",
    "# Get predicted probabilities\n",
    "y_pred_probs = model_sigmoid.predict(validation_set)\n",
    "\n",
    "# Plot histogram\n",
    "plt.figure(figsize=(5, 4))\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.histplot(\n",
    "    y_pred_probs[:, 0], bins=20, kde=True, color=\"green\", alpha=0.6, label=\"Healthy\"\n",
    ")\n",
    "sns.histplot(\n",
    "    y_pred_probs[:, 0], bins=20, kde=True, color=\"blue\", alpha=0.6, label=\"Infected\"\n",
    ")\n",
    "\n",
    "plt.axvline(x=0.5, color=\"red\", linestyle=\"dashed\", label=\"Threshold = 0.5\")\n",
    "plt.title(\"Prediction Probability Histogram\")\n",
    "plt.xlabel(\"Predicted Probability\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.legend()\n",
    "\n",
    "# Save figure\n",
    "plt.savefig(\"outputs/draft_sigmoid/histogram_test.png\", bbox_inches=\"tight\", dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting & Generalization Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the last recorded training & validation metrics\n",
    "train_acc = history_sigmoid.history[\"accuracy\"][-1]\n",
    "val_acc = history_sigmoid.history[\"val_accuracy\"][-1]\n",
    "train_loss = history_sigmoid.history[\"loss\"][-1]\n",
    "val_loss = history_sigmoid.history[\"val_loss\"][-1]\n",
    "\n",
    "# Compute Generalization Gap\n",
    "accuracy_gap = train_acc - val_acc\n",
    "loss_gap = val_loss - train_loss\n",
    "\n",
    "print(\"\\n### Generalization & Overfitting Check ###\")\n",
    "print(f\"Final Train Accuracy: {train_acc:.4f}\")\n",
    "print(f\"Final Validation Accuracy: {val_acc:.4f}\")\n",
    "print(f\"Accuracy Gap: {accuracy_gap:.4f}\")\n",
    "\n",
    "print(f\"Final Train Loss: {train_loss:.4f}\")\n",
    "print(f\"Final Validation Loss: {val_loss:.4f}\")\n",
    "print(f\"Loss Gap: {loss_gap:.4f}\")\n",
    "\n",
    "# Overfitting Analysis\n",
    "if accuracy_gap > 0.05:\n",
    "    print(\n",
    "        \"\\nOverfitting detected: The model performs significantly better on training data than validation data.\"\n",
    "    )\n",
    "\n",
    "if loss_gap > 0.05:\n",
    "    print(\n",
    "        \"\\nOverfitting detected: Validation loss is significantly higher than training loss.\"\n",
    "    )\n",
    "\n",
    "if accuracy_gap < 0.05 and loss_gap < 0.05:\n",
    "    print(\"\\nNo significant overfitting detected. Model generalizes well.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "output_dir = \"outputs/draft_sigmoid\"\n",
    "\n",
    "# Get Class Labels\n",
    "label_map = list(test_set.class_indices.keys())\n",
    "\n",
    "# Evaluate Model on Train and Test Sets\n",
    "y_true_train = train_set.classes\n",
    "y_pred_train = (model_sigmoid.predict(train_set) > 0.5).astype(int)\n",
    "\n",
    "y_true_test = test_set.classes\n",
    "y_pred_test = (model_sigmoid.predict(test_set) > 0.5).astype(int)\n",
    "\n",
    "# Generate Confusion Matrices\n",
    "cm_train = confusion_matrix(y_true_train, y_pred_train)\n",
    "cm_test = confusion_matrix(y_true_test, y_pred_test)\n",
    "\n",
    "# Plot Confusion Matrices Side by Side\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "sns.heatmap(\n",
    "    pd.DataFrame(cm_train, index=label_map, columns=label_map),\n",
    "    annot=True,\n",
    "    fmt=\"d\",\n",
    "    cmap=\"Blues\",\n",
    "    linewidths=0.5,\n",
    "    ax=axes[0],\n",
    ")\n",
    "axes[0].set_title(\"Confusion Matrix - Train Set\")\n",
    "axes[0].set_xlabel(\"Predicted Label\")\n",
    "axes[0].set_ylabel(\"True Label\")\n",
    "\n",
    "sns.heatmap(\n",
    "    pd.DataFrame(cm_test, index=label_map, columns=label_map),\n",
    "    annot=True,\n",
    "    fmt=\"d\",\n",
    "    cmap=\"Blues\",\n",
    "    linewidths=0.5,\n",
    "    ax=axes[1],\n",
    ")\n",
    "axes[1].set_title(\"Confusion Matrix - Test Set\")\n",
    "axes[1].set_xlabel(\"Predicted Label\")\n",
    "axes[1].set_ylabel(\"True Label\")\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save Figure\n",
    "save_path = os.path.join(output_dir, \"confusion_matrices_train_test.png\")\n",
    "plt.savefig(save_path, dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Confusion Matrices saved at: {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "import os\n",
    "\n",
    "\n",
    "output_dir = \"outputs/draft_sigmoid\"\n",
    "\n",
    "# Generate classification reports as text\n",
    "report_train = classification_report(\n",
    "    y_true_train, y_pred_train, target_names=label_map, digits=3, zero_division=1\n",
    ")\n",
    "report_test = classification_report(\n",
    "    y_true_test, y_pred_test, target_names=label_map, digits=3, zero_division=1\n",
    ")\n",
    "# Print side by side\n",
    "print(\"\\n### Classification Reports (Train vs Test) ###\\n\")\n",
    "train_lines = report_train.split(\"\\n\")\n",
    "test_lines = report_test.split(\"\\n\")\n",
    "\n",
    "# Align Train and Test reports side by side\n",
    "for train_line, test_line in zip(train_lines, test_lines):\n",
    "    print(f\"{train_line:<40} | {test_line}\")\n",
    "\n",
    "# Save reports as text files\n",
    "with open(f\"{output_dir}/classification_report_train.txt\", \"w\") as f:\n",
    "    f.write(report_train)\n",
    "\n",
    "with open(f\"{output_dir}/classification_report_test.txt\", \"w\") as f:\n",
    "    f.write(report_test)\n",
    "\n",
    "print(\"\\nReports saved to outputs/draft_sigmoid/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import os\n",
    "\n",
    "output_dir = \"outputs/draft_sigmoid\"\n",
    "\n",
    "# Generate predictions (probabilities)\n",
    "y_probs_train = model_sigmoid.predict(train_set)\n",
    "y_probs_test = model_sigmoid.predict(test_set)\n",
    "\n",
    "# Compute ROC curve for sigmoid model\n",
    "fpr_train, tpr_train, _ = roc_curve(y_true_train, y_probs_train)  # Remove `[:, 1]`\n",
    "fpr_test, tpr_test, _ = roc_curve(y_true_test, y_probs_test)      # Remove `[:, 1]`\n",
    "\n",
    "auc_train = auc(fpr_train, tpr_train)\n",
    "auc_test = auc(fpr_test, tpr_test)\n",
    "\n",
    "# Plot ROC Curves\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.plot(fpr_train, tpr_train, label=f\"Train AUC = {auc_train:.2f}\")\n",
    "plt.plot(fpr_test, tpr_test, label=f\"Test AUC = {auc_test:.2f}\")\n",
    "plt.plot([0, 1], [0, 1], \"k--\", label=\"Random (AUC = 0.50)\")\n",
    "\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve - Train vs Test\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "# Save figure\n",
    "roc_curve_path = os.path.join(output_dir, \"roc_curve.png\")\n",
    "plt.savefig(roc_curve_path, dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(f\"ROC Curve saved at: {roc_curve_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Final Evaluation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import os\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "output_dir = \"outputs/draft_sigmoid\"\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model.evaluate(test_set, batch_size=batch_size)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "# Get True Labels & Predictions\n",
    "y_true = test_set.classes\n",
    "y_pred_probs = model.predict(test_set)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "# Save evaluation results\n",
    "evaluation_results = {\n",
    "    \"test_loss\": test_loss,\n",
    "    \"test_accuracy\": test_accuracy,\n",
    "    \"classification_report\": classification_report(\n",
    "        y_true, y_pred, target_names=label_map, output_dict=True\n",
    "    ),\n",
    "    \"confusion_matrix\": confusion_matrix(y_true, y_pred),\n",
    "}\n",
    "\n",
    "# Save to pickle file\n",
    "joblib.dump(evaluation_results, \"outputs/draft_sigmoid/evaluation.pkl\")\n",
    "print(\"Evaluation results saved: outputs/draft_sigmoid/evaluation.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Goal Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define minimum required accuracy\n",
    "accuracy_threshold = 0.90\n",
    "\n",
    "# Load evaluation results\n",
    "evaluation_results = joblib.load(\"outputs/draft_sigmoid/evaluation.pkl\")\n",
    "\n",
    "# Extract final test accuracy\n",
    "test_accuracy = evaluation_results[\"test_accuracy\"]\n",
    "\n",
    "# Check requirement\n",
    "if test_accuracy >= accuracy_threshold:\n",
    "    print(f\"Model meets the business requirement! (Accuracy: {test_accuracy:.2%})\")\n",
    "else:\n",
    "    print(f\"Model does NOT meet the requirement. (Accuracy: {test_accuracy:.2%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict on New Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "# Load the Model\n",
    "model = load_model(\"outputs/draft_sigmoid/mildew_detector_sigmoid.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select and Load a Random Test Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define test image selection parameters\n",
    "pointer = 60  # Change this number to select a different image\n",
    "label = labels[1]  # Select \"Healthy\" (0) or \"Infected\" (1)\n",
    "\n",
    "# Load the image using PIL\n",
    "img_path = test_path + \"/\" + label + \"/\" + os.listdir(test_path + \"/\" + label)[pointer]\n",
    "pil_image = image.load_img(img_path, target_size=image_shape, color_mode=\"rgb\")\n",
    "\n",
    "# Display image details\n",
    "print(f\"Selected Image Path: {img_path}\")\n",
    "print(f\"Image shape: {pil_image.size}, Image mode: {pil_image.mode}\")\n",
    "\n",
    "# Show the image\n",
    "pil_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Image to Array and Prepare for Model Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_image = image.img_to_array(pil_image)\n",
    "my_image = np.expand_dims(my_image, axis=0) / 255.0  # Normalize pixel values\n",
    "print(my_image.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Prediction & Display Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict class probabilities\n",
    "pred_proba = model.predict(my_image)[0, 0]  # Extract single probability score\n",
    "\n",
    "# Map indices to class labels\n",
    "target_map = {v: k for k, v in train_set.class_indices.items()}  # Reverse mapping\n",
    "pred_class = target_map[int(pred_proba > 0.5)]  # Ensure correct label mapping\n",
    "\n",
    "# Adjust probability if necessary\n",
    "if pred_class == target_map[0]:\n",
    "    pred_proba = 1 - pred_proba\n",
    "\n",
    "# Print prediction results\n",
    "print(f\"Predicted Class: {pred_class}\")\n",
    "print(f\"Prediction Probability: {pred_proba:.4f}\")\n",
    "\n",
    "# Save the image to outputs/draft for PDF report\n",
    "os.makedirs(\"outputs/draft_sigmoid\", exist_ok=True)\n",
    "pil_image.save(\"outputs/draft_sigmoid/selected_test_image.png\")\n",
    "\n",
    "# Save prediction results as a text file\n",
    "with open(\"outputs/draft_sigmoid/prediction_result.txt\", \"w\") as f:\n",
    "    f.write(f\"Predicted Class: {pred_class}\\n\")\n",
    "    f.write(f\"Prediction Probability: {pred_proba:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion and Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We successfully developed a deep learning model for image classification using a structured, beginner-friendly approach.  \n",
    "\n",
    "### **Key Achievements**\n",
    "- **Baseline & Optimized CNNs** → Established a benchmark model and improved it through manual hyperparameter tuning.  \n",
    "- **Comprehensive Evaluation** → Assessed performance using accuracy, loss, confusion matrices, and ROC curves.  \n",
    "- **Model Explainability** → Utilized evaluation metrics to understand predictions and ensure reliability.  \n",
    "- **Final Model Selection** → Chose the best-performing model for deployment.  \n",
    "\n",
    "### **Next Steps: Model Deployment**\n",
    "- **Web App Integration** → Implement a user-friendly Streamlit interface for real-time image classification.  \n",
    "- **Model Deployment** → Load the trained model and deploy it on a cloud platform for accessibility.  \n",
    "\n",
    "This deployment will enable efficient real-world usage, making automated classification accessible to users.  "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Data Practitioner Jupyter Notebook.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "3.12.1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
